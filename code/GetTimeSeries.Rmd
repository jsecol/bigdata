---
title: "Solr time series data"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#### Terminal:

sudo service solr start

*If desired:*
sudo service solr status

*To see in Browser:*
http://localhost:8983/solr/

#### Load packages

```{r}
library(solrium)
library(readr)

```

#### First step

```{r}
cli <- SolrClient$new()
cli

```

#### Get time series data to upload to Solr

*manual example:*

https://docs.opendata.aws/noaa-ghcn-pds/readme.html


```{r}

X1788 <- read_csv("http://noaa-ghcn-pds.s3.amazonaws.com/csv/1788.csv", 
                  col_names = FALSE)

head(X1788)

```

#### Get station data (saved)

```{r}
read_table("http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt", 
           col_names = FALSE,
           n_max = 10)

```


```{r}
# # Fixed Width, but not needed apparently:
# read_fwf("http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt")

if(!file.exists("../data/stations.csv")) {
stations <- read_table("http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt", 
                       col_names = FALSE)
write_csv(stations, "../data/stations.csv")
} else {
stations <- read_csv("../data/stations.csv")
}

```


#### Now get 10 years of data

*works, but tried with more recent data and (too) slow*
```{r message=FALSE}

years <- 1788:1797

year_dat <- list()

for(i in 1:length(years)){
year <- paste0("http://noaa-ghcn-pds.s3.amazonaws.com/csv/", years[i], ".csv")
dat <- read_csv(year, col_names = FALSE)
year_dat[[i]] <- dat
}

names(year_dat) <- paste0("y", years)

```

*try the compressed data, as in:*
http://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/1788.csv.gz

*followed data.table suggestion here, but dont need to have curl installed (just rtools which fread in data.table needs)*
  + rtools can be installed using install.packages('rtools') apparently
  
https://stackoverflow.com/questions/3053833/using-r-to-download-zipped-data-file-extract-and-import-data


```{r}
library(data.table)
dt <- fread("http://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/1788.csv.gz")

```

*but hangs and aborts with larger, more recent, so could simply download first*

some issue here, tempfile works for csv 
```{r}
tfile <- tempfile()
download.file("http://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/1788.csv.gz", tfile)
dt <- fread(tfile)
rm(tfile)

```


