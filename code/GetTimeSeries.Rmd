---
title: "Solr time series data"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#### Terminal:

*ctrl alt enter works here*

sudo service solr start

*didn't do anything*
sudo service solr start -e cloud

*If desired, enter "q" to exit:*
sudo service solr status

*To see in Browser:*
http://localhost:8983/solr/

#### Load packages

```{r}
library(solrium)
library(readr)

```

#### First step

```{r}
cli <- SolrClient$new()
cli

```

#### Get time series data to upload to Solr

*manual example:*

https://docs.opendata.aws/noaa-ghcn-pds/readme.html


```{r}

X1788 <- read_csv("http://noaa-ghcn-pds.s3.amazonaws.com/csv/1788.csv", 
                  col_names = FALSE)

head(X1788)

```

#### Get station data (saved)

```{r}
read_table("http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt", 
           col_names = FALSE,
           n_max = 10)

```


```{r}
# # Fixed Width, but not needed apparently:
# read_fwf("http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt")

if(!file.exists("../data/stations.csv")) {
stations <- read_table("http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt", 
                       col_names = FALSE)
write_csv(stations, "../data/stations.csv")
} else {
stations <- read_csv("../data/stations.csv")
}

```


#### Now get 10 years of data

*works, but tried with more recent data and (too) slow*
```{r message=FALSE}

years <- 1788:1797

year_dat <- list()

for(i in 1:length(years)){
year <- paste0("http://noaa-ghcn-pds.s3.amazonaws.com/csv/", years[i], ".csv")
dat <- read_csv(year, col_names = FALSE)
year_dat[[i]] <- dat
}

names(year_dat) <- paste0("y", years)

```

*try the compressed data, as in:*
http://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/1788.csv.gz

*followed data.table suggestion here, but dont need to have curl installed (just rtools which fread in data.table needs)*
  + rtools can be installed using install.packages('rtools') apparently
  
https://stackoverflow.com/questions/3053833/using-r-to-download-zipped-data-file-extract-and-import-data


```{r}
library(data.table)
dt <- fread("http://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/1788.csv.gz")

```

*but hangs and aborts with larger, more recent, so could simply download first*

```{r}

# tdir <- tempdir()
# download.file("http://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/1788.csv.gz", 
#               mode = "wb", tfile)

download.file("http://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/1988.csv.gz", 
              destfile = paste0(tdir, "/1988.csv.gz"), 
              method = "wget", 
              quiet = TRUE)

# works as a test
# readLines(paste0(tdir, "/1988.csv.gz"), 1)

# crashes
# length(readLines(paste0(tdir, "/1988.csv.gz")))


file <- file(paste0("/tmp/RtmpsA5fjQ", "/1988.csv.gz"), "r")
readsizeof <- 100000
nooflines <- 0
( while((linesread <- length(readLines(file, readsizeof))) > 0 )
nooflines <- nooflines+linesread )
close(file)
nooflines

```

#### Looks like need to unzip

*but best solution is to unzip then load directly into solr?*

```{r}

# quick, but 1988 is not!
fread(paste0("/tmp/RtmpsA5fjQ", "/1788.csv.gz"), nrows = 1)

# quick, but not dataframe
chunk <- readLines(file, n = 10)
chunk



```

sudo su - solr -c "/opt/solr/bin/solr create -c y1788 -n data_driven_schema_configs"

```{r}

# # needs to be in cloud mode apparently
# if (!cli$collection_exists("y1788")) {
#   cli$collection_create(name = "y1788")
# }

library(R.utils)
gunzip(paste0("/tmp/RtmpsA5fjQ", "/1788.csv.gz"), remove = FALSE)

cli$update_csv(files = paste0("/tmp/RtmpsA5fjQ", "/1788.csv"), 
               name = "y1788",
               separator = ",", 
               header = FALSE,
               fieldnames = c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8"))

tmp <- cli$search(name = "y1788", list(q = "*:*"))

tmp <- solr_search(cli, "y1788")

```



```{r}

# df <- data.frame(id=1:3, name=c('red', 'blue', 'green'))
# write.csv(df, file="df.csv", row.names=FALSE, quote = FALSE)
# cli$update_csv("df.csv", "y1788", verbose = TRUE)

# # needs to be in cloud mode also
collection_delete(cli, "y1788")


```

