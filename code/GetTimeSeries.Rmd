---
title: "Solr time series data"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


#### Terminal:

*ctrl alt enter works here*

sudo service solr start

*didn't do anything*
sudo service solr start -e cloud

*If desired, enter "q" to exit:*
sudo service solr status

*To see in Browser:*
http://localhost:8983/solr/

#### Load packages

```{r}
library(solrium)
# library(readr)

```

#### First step

```{r}
cli <- SolrClient$new()
cli

```

#### Get time series data to upload to Solr

*manual example:*

https://docs.opendata.aws/noaa-ghcn-pds/readme.html


```{r}

X1788 <- read_csv("http://noaa-ghcn-pds.s3.amazonaws.com/csv/1788.csv", 
                  col_names = FALSE)

head(X1788)

```

#### Get station data (saved)

```{r}
read_table("http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt", 
           col_names = FALSE,
           n_max = 10)

```


```{r}
# # Fixed Width, but not needed apparently:
# read_fwf("http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt")

if(!file.exists("../data/stations.csv")) {
stations <- read_table("http://noaa-ghcn-pds.s3.amazonaws.com/ghcnd-stations.txt", 
                       col_names = FALSE)
write_csv(stations, "../data/stations.csv")
} else {
stations <- read_csv("../data/stations.csv")
}

```


#### Now get 10 years of data

*works, but tried with more recent data and (too) slow*
```{r message=FALSE}

years <- 1788:1797

year_dat <- list()

for(i in 1:length(years)){
year <- paste0("http://noaa-ghcn-pds.s3.amazonaws.com/csv/", years[i], ".csv")
dat <- read_csv(year, col_names = FALSE)
year_dat[[i]] <- dat
}

names(year_dat) <- paste0("y", years)

```

*try the compressed data, as in:*
http://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/1788.csv.gz

*followed data.table suggestion here, but dont need to have curl installed (just rtools which fread in data.table needs)*
  + rtools can be installed using install.packages('rtools') apparently
  
https://stackoverflow.com/questions/3053833/using-r-to-download-zipped-data-file-extract-and-import-data


```{r}
library(data.table)
dt <- fread("http://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/1788.csv.gz")

```

*but hangs and aborts with larger, more recent, so could simply download first*

```{r}

# tdir <- tempdir()
# download.file("http://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/1788.csv.gz", 
#               mode = "wb", tfile)

# dir.create("~/Data")
saved_dir <- "~/Data"

if(!file.exists(paste0(saved_dir, "/2010.csv.gz"))) {
download.file("http://noaa-ghcn-pds.s3.amazonaws.com/csv.gz/2010.csv.gz", 
              destfile = paste0(saved_dir, "/2010.csv.gz"), 
              method = "wget", 
              quiet = TRUE)
}

# works as a test
# readLines(paste0(tdir, "/1988.csv.gz"), 1)

# crashes
# length(readLines(paste0(tdir, "/1988.csv.gz")))


file <- file(paste0(saved_dir, "/2010.csv.gz"), "r")
readsizeof <- 200000
nooflines <- 0
( while((linesread <- length(readLines(file, readsizeof))) > 0 )
nooflines <- nooflines+linesread )
close(file)
nooflines

```

#### Looks like need to unzip

*but best solution is to unzip then load directly into solr?*

```{r}

# 1788.csv.gz quick, but 1988 is not!
fread(paste0(saved_dir, "/1788.csv.gz"), nrows = 1)

# quick, but not dataframe
chunk <- readLines(paste0(saved_dir, "/2010.csv.gz"), n = 1)
chunk

```

#### Read in large file in chunks

*try this in loop with adding to solr in chunks, check speed*

```{r}

library(R.utils)
# gunzip(paste0(saved_dir, "/2010.csv.gz"), remove = FALSE)

# test
chunk1 <- fread(paste0(saved_dir, "/2010.csv"), nrows = 1000000, skip = 0)
chunk2 <- fread(paste0(saved_dir, "/2010.csv"), nrows = 1000000, skip = 1000000)



```





sudo su - solr -c "/opt/solr/bin/solr create -c y2010 -n data_driven_schema_configs"

```{r}

# # needs to be in cloud mode apparently
# if (!cli$collection_exists("y1788")) {
#   cli$collection_create(name = "y1788")
# }

# try just one line with right types
df <- data.frame(stringsAsFactors = FALSE, 
                 V1 = "Z", V2 = as.integer(0), V3 = "Z", V4 = as.integer(0), 
                 V5 = "Z", V6 = "Z", V7 = "Z", V8 = "Z")

# # worked
# cli$add(df, "y2010")


```

#### Next try uploading chunks and add to collection (not the whole thing for now)
```{r}

# cli$update_csv(files = paste0(saved_dir, "/2010.csv.gz"), 
#                name = "y2010",
#                separator = ",", 
#                header = FALSE,
#                fieldnames = c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8"))

tmp <- cli$search(name = "y2010", list(q = "*:*"))

tmp <- solr_search(cli, "y2010")

tmp
```



```{r}

# df <- data.frame(id=1:3, name=c('red', 'blue', 'green'))
# write.csv(df, file="df.csv", row.names=FALSE, quote = FALSE)
# cli$update_csv("df.csv", "y1788", verbose = TRUE)

# # needs to be in cloud mode also
collection_delete(cli, "y2010")


```

